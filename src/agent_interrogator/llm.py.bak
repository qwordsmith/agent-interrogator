"""LLM interface implementations."""

import json
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional

from openai import AsyncOpenAI
from transformers import pipeline

from .config import InterrogationConfig, LLMConfig, ModelProvider
from .models import Function, Parameter
from .prompt_templates import *  # Import all prompt templates


class LLMInterface(ABC):
    """Abstract base class for LLM implementations."""
    
    def __init__(self, config: InterrogationConfig):
        """Initialize the LLM interface.
        
        Args:
            config: Full interrogation configuration
        """
        self.config = config
    
    @abstractmethod
    async def generate_prompt(self, context: Dict[str, Any]) -> str:
        """Generate an interrogation prompt based on context.
        
        Args:
            context: Dictionary containing:
                - phase: str, one of 'discovery', 'analysis'
                - cycle: int, current iteration number
                - previous_responses: list of previous responses
                - discovered_capabilities: list of capabilities found so far
                - capability: dict, current capability being analyzed (for analysis phase)
                - discovered_functions: list of functions found so far (for analysis phase)
        """
        pass

    @abstractmethod
    async def process_discovery_response(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Process the agent's response during capability discovery phase.
        
        Args:
            response: Raw response from the agent
            context: Current conversation context
            
        Returns:
            Dict containing:
            - capabilities: List of discovered capabilities
            - next_cycle_focus: Optional guidance for next discovery cycle
        """
        pass
    
    @abstractmethod
    async def process_analysis_response(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Process the agent's response during capability analysis phase.
        
        Args:
            response: Raw response from the agent
            context: Current conversation context with capability being analyzed
            
        Returns:
            Dict containing:
            - functions: List of discovered functions
            - next_cycle_focus: Optional guidance for next analysis cycle
        """
        pass
        
    async def process_response(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Process the agent's response based on the current phase.
        
        Args:
            response: Raw response from the agent
            context: Current conversation context
            
        Returns:
            Structured data extracted from the response
        """
        phase = context.get("phase", "discovery")
        if phase == "discovery":
            return await self.process_discovery_response(response, context)
        else:
            return await self.process_analysis_response(response, context)


class OpenAILLM(LLMInterface):
    """OpenAI-based LLM implementation."""
    
    def __init__(self, config: InterrogationConfig):
        super().__init__(config)
        if config.llm.provider != ModelProvider.OPENAI:
            raise ValueError("Invalid provider for OpenAILLM")
            
        # Configure OpenAI client
        self.client = AsyncOpenAI(api_key=config.llm.api_key)
        self.model_kwargs = config.llm.model_kwargs

    async def generate_prompt(self, context: Dict[str, Any]) -> str:
        """Generate an interrogation prompt based on context."""
        phase = context.get("phase", "discovery")
        cycle = context.get("cycle", 0)
        
        if phase == "discovery":
            # Initial discovery prompt
            if cycle == 0:
                return INITIAL_DISCOVERY_PROMPT
            
            # Follow-up discovery prompts
            discovered = context.get("discovered_capabilities", [])
            next_focus = context.get("next_cycle_focus")
            
            if cycle == 0:
                prompt = f"""
Let's analyze the '{capability.name}' capability in detail.
What specific functions or methods does it provide? For each one, please describe:
1. The function name and purpose
2. Required and optional parameters
3. Return value and type
4. Any constraints or limitations"""
            else:
                prompt = f"""
Regarding the '{capability.name}' capability:

Known functions:
{functions_str}

{f'Based on previous analysis, I should focus on: {next_focus}' if next_focus else ''}

What other functions or methods does this capability provide? Please describe any that weren't mentioned before."""
            
            return prompt
            capabilities_str = "\n".join(f"- {cap.name}: {cap.description or ''}" 
                                    for cap in discovered)
            
            prompt = f"""


    async def process_discovery_response(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Process the agent's response during capability discovery phase."""
        print("Discovery Response:")
        print(response)
        
        # Use templates from prompt_templates.py
        discovery_prompt = DISCOVERY_PROCESSING_PROMPT_TEMPLATE.format(
            json_format=DISCOVERY_JSON_SCHEMA,
            response=response
        )
        
        discovery = await self.client.chat.completions.create(
            model=self.config.llm.model_name,
            messages=[
                {
                    "role": "system",
                    "content": DISCOVERY_PROCESSING_SYSTEM_PROMPT
                },
                {"role": "user", "content": discovery_prompt}
            ],
            temperature=0.1,
            **self.model_kwargs
        )
        
        try:
            return json.loads(discovery.choices[0].message.content)
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to process discovery response: {str(e)}")

    async def process_analysis_response(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Process the agent's response during capability analysis phase."""
        print("Analysis Response:")
        print(response)
        
        capability = context["capability"]
        
        # Use templates from prompt_templates.py
        analysis_prompt = ANALYSIS_PROCESSING_PROMPT_TEMPLATE.format(
            capability_name=capability.name,
            json_format=ANALYSIS_JSON_SCHEMA,
            response=response
        )
        
        analysis = await self.client.chat.completions.create(
            model=self.config.llm.model_name,
            messages=[
                {
                    "role": "system",
                    "content": ANALYSIS_PROCESSING_SYSTEM_PROMPT
                },
                {"role": "user", "content": analysis_prompt}
            ],
            temperature=0.1,
            **self.model_kwargs
        )
        
        try:
            return json.loads(analysis.choices[0].message.content)
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to process analysis response: {str(e)}")

    async def should_continue_cycle(self, context: Dict[str, Any], results: Dict[str, Any]) -> bool:
        """Determine if another cycle should be run based on results."""
        # Stop if marked complete
        if results.get("is_complete", False):
            return False
            
        # Stop if no new discoveries
        if not results.get("capabilities", []) and not results.get("functions", []):
            return False
            
        # Stop if max cycles reached
        max_cycles = 5  # Could be made configurable
        if context["cycle"] >= max_cycles - 1:
            return False
            
        return True
        
    def _format_known_items(self, context: Dict[str, Any]) -> str:
        """Format known capabilities and functions for prompt context."""
        # Format capabilities
        capabilities = context.get("discovered_capabilities", [])
        capabilities_str = "\n".join(f"- {cap.name}: {cap.description or ''}" 
                                for cap in capabilities)
        
        # Format functions
        functions = context.get("discovered_functions", [])
        functions_str = "\n".join(
            f"- {func.name}: {func.description or ''}" 
            for func in functions
        )
        
        return f"""Known Capabilities:
{capabilities_str if capabilities_str else 'None'}

Known Functions:
{functions_str if functions_str else 'None'}"""
            else:
                prompt = f"""
Regarding the '{capability.name}' capability:

Known functions:
{functions_str}

{f'Based on previous analysis, I should focus on: {next_focus}' if next_focus else ''}

What other functions are available? Please provide full details about any additional methods."""
            
            return prompt

    async def process_discovery_response(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Process the agent's response during capability discovery phase."""
        print("Discovery Response:")
        print(response)
        
        json_format = r'''{
    "capabilities": [{ 
        "name": "capability name", 
        "description": "detailed description of what this capability enables" 
    }],
    "next_cycle_focus": "guidance for what aspects to explore in the next cycle"
}'''
        
        analysis_prompt = f"""
Analyze the following agent response and extract structured information about its capabilities.
Focus on identifying distinct, well-defined capabilities and filter out any hallucinations or unsupported claims.

Format the output as JSON following this schema:
{json_format}

Agent Response:
{response}
"""
        
        analysis = await self.client.chat.completions.create(
            model=self.config.llm.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert at identifying and categorizing AI agent capabilities."
                },
                {"role": "user", "content": analysis_prompt}
            ],
            temperature=0.1,
            **self.model_kwargs
        )
        
        try:
            return json.loads(analysis.choices[0].message.content)
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to process discovery response: {str(e)}")

    async def process_analysis_response(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Process the agent's response during capability analysis phase."""
        print("Analysis Response:")
        print(response)
        
        json_format = r'''{
    "functions": [{ 
        "name": "function name", 
        "description": "function description", 
        "parameters": [ 
            { 
                "name": "param1", 
                "type": "string", 
                "description": "Description of param1", 
                "required": true 
            } 
        ], 
        "return_type": "string" 
    }],
    "next_cycle_focus": "guidance for what aspects to analyze in the next cycle"
}'''
        
        capability = context["capability"]
        analysis_prompt = f"""
Analyze the following agent response and extract structured information about the functions within the '{capability.name}' capability.
Focus on accurately capturing function signatures, parameters, and return types.

Format the output as JSON following this schema:
{json_format}

Agent Response:
{response}
"""
        
        analysis = await self.client.chat.completions.create(
            model=self.config.llm.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert at analyzing and documenting API functions and parameters."
                },
                {"role": "user", "content": analysis_prompt}
            ],
            temperature=0.1,
            **self.model_kwargs
        )
        
        try:
            return json.loads(analysis.choices[0].message.content)
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to process analysis response: {str(e)}")

    async def should_continue_cycle(self, context: Dict[str, Any], results: Dict[str, Any]) -> bool:
        """Determine if another cycle should be run based on results."""
        # Stop if marked complete
        try:
            return json.loads(analysis.choices[0].message.content)
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to process discovery response: {str(e)}")
    
    async def process_analysis_response(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        print("Analysis Response:")
        print(response)
        
        # Template for function analysis
        json_format = r'''{
"functions": [{ 
    "name": "function name", 
    "description": "function description", 
    "parameters": [ 
        { 
            "name": "param1", 
            "type": "string", 
            "description": "Description of param1", 
            "required": true 
        } 
    ], 
    "return_type": "string" 
}],
"next_cycle_focus": "guidance for what aspects to analyze in the next cycle"
}'''
        
        capability = context["capability"]
        analysis_prompt = f"""
Analyze the following agent response and extract structured information about the functions within the '{capability.name}' capability.
Focus on accurately capturing function signatures, parameters, and return types.

Format the output as JSON following this schema:
{json_format}
                f"- {item['name'] if isinstance(item, dict) else item.name}: "
                f"{item.get('description', '') if isinstance(item, dict) else (item.description or '')}"
                for item in items
            )
        else:
            functions = context["discovered_functions"]
            return "\n".join(
                f"- {item['name'] if isinstance(item, dict) else item.name}: "
                f"{item.get('description', '') if isinstance(item, dict) else (item.description or '')}"
                for item in functions
            )

    async def should_continue_cycle(self, context: Dict[str, Any], results: Dict[str, Any]) -> bool:
        # Stop if explicitly marked as complete
        if results.get("is_complete", False):
            return False
            
        # Stop if max cycles reached
        max_cycles = self.config.max_iterations
        if context["cycle"] >= max_cycles - 1:
            return False
            
        return True


class HuggingFaceLLM(LLMInterface):
    """HuggingFace implementation of the LLM interface."""
    
    def __init__(self, config: InterrogationConfig):
        super().__init__(config)
        
        if config.llm.provider != ModelProvider.HUGGINGFACE:
            raise ValueError("Invalid provider for HuggingFaceLLM")
        
        # Initialize generation model
        self.gen_model = pipeline(
            "text-generation",
            model=config.llm.model_name,
            **config.llm.model_kwargs
        )
        
        # Initialize a separate model for analysis (can be same or different)
        self.analysis_model = pipeline(
            "text-generation",
            model=config.llm.model_name,
            **{**config.llm.model_kwargs, "temperature": 0.1}  # Lower temperature for analysis
        )
        
    def _build_system_prompt(self, context: Dict[str, Any]) -> str:
        phase = context["phase"]
        cycle = context["cycle"]
        previous_responses = context["previous_responses"]
        
        if phase == "discovery":
            return f"""Task: Generate a prompt to interrogate an AI agent (cycle {cycle}).

Context:
- Found {len(context['discovered_capabilities'])} capabilities
- {len(previous_responses)} previous interactions

Create a clear prompt to discover ALL tools and APIs.
Build on previous responses. Focus on unexplored areas."""
        else:
            capability = context["capability"]
            return f"""Task: Generate a prompt to analyze '{capability.name}' (cycle {cycle}).

Context:
- Found {len(context['discovered_functions'])} functions
- {len(previous_responses)} previous interactions

Create a technical prompt to uncover ALL functions and features.
Build on previous responses. Focus on details."""

    def _format_previous_responses(self, responses: list[str]) -> str:
        if not responses:
            return "No previous interactions."
            
        summary = []
        for i, resp in enumerate(responses):
            # Keep summaries very brief for HuggingFace models
            resp_summary = resp[:100] + "..." if len(resp) > 100 else resp
            summary.append(f"R{i+1}: {resp_summary}")
            
        return "\n".join(summary)

    def _build_discovery_prompt(self, context: Dict[str, Any]) -> str:
        cycle = context["cycle"]
        discovered = context["discovered_capabilities"]
        previous_responses = context["previous_responses"]
        
        if cycle == 0:
            return """Create a prompt that asks the agent to:
1. List ALL tools and APIs
2. Describe each capability
3. Group related capabilities"""
        else:
            caps = "\n".join(f"- {c.name}" for c in discovered)
            responses = self._format_previous_responses(previous_responses)
            
            return f"""Known capabilities:
{caps}

Previous responses:
{responses}

Create a prompt to:
1. Find new capabilities
2. Clarify existing ones
3. Check for hidden features"""

    def _build_analysis_prompt(self, context: Dict[str, Any]) -> str:
        cycle = context["cycle"]
        capability = context["capability"]
        discovered = context["discovered_functions"]
        previous_responses = context["previous_responses"]
        
        if cycle == 0:
            return f"""Create a prompt to analyze '{capability.name}'.
Ask about:
1. Available functions
2. Parameters and types
3. Return values
4. Usage limits
5. Special modes"""
        else:
            funcs = "\n".join(f"- {func.name}: {func.description or ''}" for func in discovered)
            responses = self._format_previous_responses(previous_responses)
            
            return f"""Known functions in '{capability.name}':
{funcs}

Previous responses:
{responses}

Create a prompt to:
1. Find new functions
2. Get implementation details
3. Check error handling
4. Verify constraints"""
            
    async def generate_prompt(self, context: Dict[str, Any]) -> str:
        """Generate an interrogation prompt based on context."""
        phase = context.get("phase", "discovery")
        cycle = context.get("cycle", 0)
        
        if phase == "discovery":
            if cycle == 0:
                return "What capabilities and tools do you have? Please describe each one in detail."
            
            discovered = context.get("discovered_capabilities", [])
            next_focus = context.get("next_cycle_focus")
            
            capabilities_str = "\n".join(f"- {cap.name}: {cap.description or ''}" 
                                    for cap in discovered)
            
            prompt = f"""
I've learned about these capabilities so far:
{capabilities_str}

{f'Based on previous exploration, I should focus on: {next_focus}' if next_focus else ''}

What other capabilities do you have? Please describe any additional tools or APIs that weren't mentioned before."""
            
            return prompt
        else:
            capability = context["capability"]
            discovered_functions = context.get("discovered_functions", [])
            next_focus = context.get("next_cycle_focus")
            
            functions_str = "\n".join(f"- {func.name}: {func.description or ''}" 
                                for func in discovered_functions)
            
            if cycle == 0:
                prompt = f"""
Let's analyze the '{capability.name}' capability in detail.
What specific functions or methods does it provide? For each one, please describe:
1. The function name and purpose
2. Required and optional parameters
3. Return value and type
4. Any constraints or limitations"""
            else:
                prompt = f"""
Regarding the '{capability.name}' capability:

Known functions:
{functions_str}

{f'Based on previous analysis, I should focus on: {next_focus}' if next_focus else ''}

What other functions or methods does this capability provide? Please describe any that weren't mentioned before."""
            
            return prompt


    async def process_analysis_response(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        print("Analysis Response:")
        print(response)
        
        # Template for function analysis
        json_format = r'''{
    "functions": [{ 
        "name": "function name", 
        "description": "function description", 
        "parameters": [ 
            { 
                "name": "param1", 
                "type": "string", 
                "description": "Description of param1", 
                "required": true 
            } 
        ], 
        "return_type": "string" 
    }],
    "next_cycle_focus": "guidance for what aspects to analyze in the next cycle"
}'''
        
        capability = context["capability"]
        analysis_prompt = f"""
Analyze the following agent response and extract structured information about the functions within the '{capability.name}' capability.
Focus on accurately capturing function signatures, parameters, and return types.

Format the output as JSON following this schema:
{json_format}

Agent Response:
{response}
"""
        
        analysis = await self.client.chat.completions.create(
            model=self.config.llm.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert at analyzing and documenting API functions and parameters."
                },
                {"role": "user", "content": analysis_prompt}
            ],
            temperature=0.1,
            **self.model_kwargs
        )
        
        try:
            return json.loads(analysis.choices[0].message.content)
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to process analysis response: {str(e)}")


    def _format_known_items(self, context: Dict[str, Any]) -> str:
        """Format known capabilities and functions for prompt context."""
        # Format capabilities
        capabilities = context.get("discovered_capabilities", [])
        capabilities_str = "\n".join(f"- {cap.name}: {cap.description or ''}" 
                                for cap in capabilities)
        
        # Format functions
        functions = context.get("discovered_functions", [])
        functions_str = "\n".join(
            f"- {func.name}: {func.description or ''}" 
            for func in functions
        )
        
        return f"""Known Capabilities:
{capabilities_str if capabilities_str else 'None'}

Known Functions:
{functions_str if functions_str else 'None'}"""

    async def should_continue_cycle(self, context: Dict[str, Any], results: Dict[str, Any]) -> bool:
        """Determine if another cycle should be run based on results."""
        # Stop if marked complete
        if results.get("is_complete", False):
            return False
            
        # Stop if no new discoveries
        if not results.get("capabilities", []) and not results.get("functions", []):
            return False
            
        # Stop if max cycles reached
        max_cycles = 5  # Could be made configurable
        if context["cycle"] >= max_cycles - 1:
            return False
            
        return True


